{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHFiTnPijAT",
        "outputId": "f7d569db-ac92-402c-aa21-a404af4f2b91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "YG8N8MhziYvw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f0DfJq4PiWIs"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_conv_layers, n_filters, n_fc_layers, n_neurons, dropout_rate):\n",
        "        super(Net, self).__init__()\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        image_size = 32\n",
        "\n",
        "        # Add convolutional layers\n",
        "        for i in range(n_conv_layers):\n",
        "            layers.append(nn.Conv2d(in_channels, n_filters, kernel_size=3, padding=1))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.MaxPool2d(2, 2))\n",
        "            in_channels = n_filters\n",
        "\n",
        "        # Calculate the final image size after convolution and pooling\n",
        "        image_size = image_size // (2 ** n_conv_layers)\n",
        "\n",
        "        # Add fully connected layers\n",
        "        layers.append(nn.Flatten())\n",
        "        in_features = n_filters * image_size * image_size  # Update in_features for the first linear layer\n",
        "        for i in range(n_fc_layers):\n",
        "            layers.append(nn.Linear(in_features, n_neurons))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(p=dropout_rate))\n",
        "            in_features = n_neurons  # Update in_features for the next layer\n",
        "\n",
        "        layers.append(nn.Linear(in_features, 100))  # 100 output classes for CIFAR-100\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Function to get the CIFAR-100 data loaders\n",
        "def get_data_loaders(batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# Function to train and evaluate the model\n",
        "def train_and_evaluate(model, optimizer, criterion, trainloader, testloader, device, epochs=10):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_total = 0\n",
        "        train_correct = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        ## Calculate accuracy for training phase\n",
        "        train_accuracy = train_correct / train_total\n",
        "        ## NORMAL running_loss over batches\n",
        "        running_loss /= len(trainloader)\n",
        "\n",
        "        model.eval()\n",
        "        test_total = 0\n",
        "        test_correct = 0\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                images, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        ## Calculate accuracy for test phase\n",
        "        test_accuracy = test_correct / test_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.3f}, TrAccuracy: {100 * train_accuracy:.2f}%, TeAccuracy: {100 * test_accuracy:.2f}\")\n",
        "\n",
        "    return 1 - test_accuracy\n",
        "\n",
        "# Define the optimization objective\n",
        "def objective(trial):\n",
        "    # Set up the hyperparameters to optimize\n",
        "    n_conv_layers = trial.suggest_int('n_conv_layers', 2, 4)\n",
        "    n_filters = trial.suggest_int('n_filters', 60, 64)\n",
        "    n_fc_layers = trial.suggest_int('n_fc_layers', 2, 3)\n",
        "    n_neurons = trial.suggest_int('n_neurons', 60, 64)\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-2)\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Get data loaders\n",
        "    trainloader, testloader = get_data_loaders(batch_size=128)\n",
        "\n",
        "    # Create the model\n",
        "    model = Net(n_conv_layers, n_filters, n_fc_layers, n_neurons, dropout_rate)\n",
        "\n",
        "    # Set up the optimizer and loss criterion\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    error_rate = train_and_evaluate(model, optimizer, criterion, trainloader, testloader, device)\n",
        "\n",
        "    return error_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "try:\n",
        "    study.optimize(objective, timeout=3600, n_jobs=1)\n",
        "except optuna.exceptions.TrialPruned as e:\n",
        "    print(\"Trial was pruned.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqKru05tie36",
        "outputId": "0071dfc2-04e4-4b51-c808-7f57a5fdbf7c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-30 15:14:12,309] A new study created in memory with name: no-name-746368f9-1105-4cf8-8d70-9cf6c06a0f60\n",
            "<ipython-input-22-dd7d9cc7d99f>:99: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
            "<ipython-input-22-dd7d9cc7d99f>:100: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10, Loss: 4.195, TrAccuracy: 5.44%, TeAccuracy: 7.05\n",
            "Epoch 2/10, Loss: 3.711, TrAccuracy: 12.15%, TeAccuracy: 17.23\n",
            "Epoch 3/10, Loss: 3.447, TrAccuracy: 16.32%, TeAccuracy: 19.27\n",
            "Epoch 4/10, Loss: 3.287, TrAccuracy: 19.35%, TeAccuracy: 23.77\n",
            "Epoch 5/10, Loss: 3.155, TrAccuracy: 21.86%, TeAccuracy: 23.58\n",
            "Epoch 6/10, Loss: 3.050, TrAccuracy: 23.79%, TeAccuracy: 26.82\n",
            "Epoch 7/10, Loss: 2.968, TrAccuracy: 25.27%, TeAccuracy: 27.84\n",
            "Epoch 8/10, Loss: 2.891, TrAccuracy: 26.46%, TeAccuracy: 29.15\n",
            "Epoch 9/10, Loss: 2.838, TrAccuracy: 27.58%, TeAccuracy: 29.96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-30 15:43:13,904] Trial 0 finished with value: 0.7001 and parameters: {'n_conv_layers': 2, 'n_filters': 63, 'n_fc_layers': 2, 'n_neurons': 62, 'dropout_rate': 0.11006712947473624, 'learning_rate': 0.0023250749854313688, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.7001.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.766, TrAccuracy: 28.88%, TeAccuracy: 29.99\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10, Loss: 4.090, TrAccuracy: 6.53%, TeAccuracy: 10.62\n",
            "Epoch 2/10, Loss: 3.587, TrAccuracy: 14.19%, TeAccuracy: 18.13\n",
            "Epoch 3/10, Loss: 3.285, TrAccuracy: 19.51%, TeAccuracy: 21.28\n",
            "Epoch 4/10, Loss: 3.084, TrAccuracy: 23.26%, TeAccuracy: 25.33\n",
            "Epoch 5/10, Loss: 2.936, TrAccuracy: 26.10%, TeAccuracy: 26.89\n",
            "Epoch 6/10, Loss: 2.820, TrAccuracy: 28.38%, TeAccuracy: 28.79\n",
            "Epoch 7/10, Loss: 2.709, TrAccuracy: 30.75%, TeAccuracy: 28.41\n",
            "Epoch 8/10, Loss: 2.629, TrAccuracy: 32.23%, TeAccuracy: 31.80\n",
            "Epoch 9/10, Loss: 2.560, TrAccuracy: 33.91%, TeAccuracy: 30.36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-30 16:13:36,170] Trial 1 finished with value: 0.6736 and parameters: {'n_conv_layers': 3, 'n_filters': 60, 'n_fc_layers': 2, 'n_neurons': 62, 'dropout_rate': 0.012727925703046539, 'learning_rate': 0.0015369773134676792, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.6736.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.494, TrAccuracy: 35.40%, TeAccuracy: 32.64\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/10, Loss: 4.306, TrAccuracy: 3.42%, TeAccuracy: 7.51\n",
            "Epoch 2/10, Loss: 3.993, TrAccuracy: 6.69%, TeAccuracy: 11.05\n",
            "Epoch 3/10, Loss: 3.843, TrAccuracy: 8.88%, TeAccuracy: 13.07\n",
            "Epoch 4/10, Loss: 3.742, TrAccuracy: 10.29%, TeAccuracy: 15.03\n",
            "Epoch 5/10, Loss: 3.655, TrAccuracy: 11.36%, TeAccuracy: 16.51\n",
            "Epoch 6/10, Loss: 3.584, TrAccuracy: 12.55%, TeAccuracy: 17.41\n",
            "Epoch 7/10, Loss: 3.536, TrAccuracy: 13.53%, TeAccuracy: 18.78\n",
            "Epoch 8/10, Loss: 3.482, TrAccuracy: 14.20%, TeAccuracy: 19.11\n",
            "Epoch 9/10, Loss: 3.457, TrAccuracy: 14.62%, TeAccuracy: 20.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-06-30 16:42:06,410] Trial 2 finished with value: 0.788 and parameters: {'n_conv_layers': 3, 'n_filters': 60, 'n_fc_layers': 2, 'n_neurons': 64, 'dropout_rate': 0.3971681002979143, 'learning_rate': 0.002120029332257018, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.6736.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 3.426, TrAccuracy: 15.02%, TeAccuracy: 21.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best hyperparameters and error rate\n",
        "best_params = study.best_params\n",
        "best_error = study.best_value\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Error Rate:\", best_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQEw2nmbig6T",
        "outputId": "4477f873-8fce-461c-d42c-57f140153779"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'n_conv_layers': 3, 'n_filters': 60, 'n_fc_layers': 2, 'n_neurons': 62, 'dropout_rate': 0.012727925703046539, 'learning_rate': 0.0015369773134676792, 'optimizer': 'RMSprop'}\n",
            "Best Error Rate: 0.6736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BC_yIK99L-tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}