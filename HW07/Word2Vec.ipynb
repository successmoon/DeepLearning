{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bGq1XjUcwcWn"
      },
      "outputs": [],
      "source": [
        "# Word2Vec Implementation\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Word2Vec model class\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, target_word, context_word):\n",
        "        target_embed = self.in_embed(target_word)\n",
        "        context_embed = self.out_embed(context_word)\n",
        "        return target_embed, context_embed"
      ],
      "metadata": {
        "id": "ihrIIAXNwgBA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, corpus, window_size):\n",
        "        self.corpus = corpus\n",
        "        self.window_size = window_size\n",
        "        self.word_pairs = self.generate_word_pairs()\n",
        "\n",
        "    def generate_word_pairs(self):\n",
        "        word_pairs = []\n",
        "        for i, center_word in enumerate(self.corpus):\n",
        "            context_words = self.corpus[max(0, i - self.window_size):i] + self.corpus[i + 1:min(len(self.corpus) - 1, i + self.window_size) + 1]\n",
        "            for context_word in context_words:\n",
        "                word_pairs.append((center_word, context_word))\n",
        "        return word_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        target_word, context_word = self.word_pairs[idx]\n",
        "        return torch.tensor(target_word), torch.tensor(context_word)\n",
        "\n",
        "def train_word2vec(corpus, window_size, embedding_dim, num_epochs, learning_rate):\n",
        "    # Preprocess the corpus and build the vocabulary\n",
        "    corpus = corpus.lower().split()\n",
        "    vocab = set(corpus)\n",
        "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "    idx2word = {i: word for i, word in enumerate(vocab)}\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    dataset = Word2VecDataset([word2idx[word] for word in corpus], window_size)\n",
        "    dataloader = DataLoader(dataset, shuffle=True)\n",
        "\n",
        "    # Create the target-context word pairs\n",
        "\n",
        "\n",
        "    # Initialize the Word2Vec model\n",
        "    model = Word2Vec(vocab_size, embedding_dim)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            target_word, context_word = data\n",
        "            target_embed, context_embed = model(torch.LongTensor([target_word]), torch.LongTensor([context_word]))\n",
        "            context_weights = model.out_embed.weight.data\n",
        "            logits = torch.matmul(context_weights, target_embed.squeeze()).unsqueeze(0) # (vocab_size)\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "            preds = softmax(logits)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(preds, torch.LongTensor([context_word]))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print the average loss for the epoch\n",
        "        print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, running_loss/len(dataloader)))\n",
        "\n",
        "    # Return the trained Word2Vec model\n",
        "    return model, word2idx, idx2word, vocab_size, vocab\n",
        "\n",
        "def cosine_similarity(embedding1, embedding2):\n",
        "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n"
      ],
      "metadata": {
        "id": "DzHH19FowgEH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main function\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set hyperparameters\n",
        "    corpus = \"I love to learn deep learning. It is fascinating!\"\n",
        "    window_size = 3\n",
        "    embedding_dim = 10\n",
        "    num_epochs = 100\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Load and preprocess the corpus\n",
        "\n",
        "    # Train the Word2Vec model\n",
        "    model, word2idx, idx2word, vocab_size, vocab = train_word2vec(corpus, window_size, embedding_dim, num_epochs, learning_rate)\n",
        "\n",
        "    # Evaluate the trained model using word similarity or analogy tasks\n",
        "    predicted_scores = []\n",
        "\n",
        "    for _item in vocab:\n",
        "      word1_idx = word2idx.get(_item)\n",
        "      print(_item + \" similarities: => \")\n",
        "      for item in vocab:\n",
        "        word2_idx = word2idx.get(item)\n",
        "        embedding1 = model.in_embed(torch.tensor(word1_idx)).detach().numpy()\n",
        "        embedding2 = model.in_embed(torch.tensor(word2_idx)).detach().numpy()\n",
        "        predicted_score = cosine_similarity(embedding1, embedding2)\n",
        "        print(item + \" \" + str(predicted_score))\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "    # Print the learned word embeddings\n",
        "    embedding_matrix = model.in_embed.weight.data.numpy()\n",
        "    for i, word in enumerate(model.in_embed.weight.data):\n",
        "        print(\"Word: {}, Embedding: {}\".format(idx2word[i], word))\n",
        "\n",
        "\n",
        "    # Save the trained model\n",
        "    drive.mount('/content/drive')\n",
        "    torch.save(model, '/content/drive/My Drive/DLAssignments/07/model.pth')\n",
        "    torch.save(model.state_dict(), '/content/drive/My Drive/DLAssignments/07/model_state_dict.pth')\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0MuMxPBQwgJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b2091c-4e9e-4794-edc3-bd2188f2790e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.2536\n",
            "Epoch [2/100], Loss: 2.2523\n",
            "Epoch [3/100], Loss: 2.2509\n",
            "Epoch [4/100], Loss: 2.2493\n",
            "Epoch [5/100], Loss: 2.2476\n",
            "Epoch [6/100], Loss: 2.2459\n",
            "Epoch [7/100], Loss: 2.2442\n",
            "Epoch [8/100], Loss: 2.2423\n",
            "Epoch [9/100], Loss: 2.2403\n",
            "Epoch [10/100], Loss: 2.2382\n",
            "Epoch [11/100], Loss: 2.2361\n",
            "Epoch [12/100], Loss: 2.2341\n",
            "Epoch [13/100], Loss: 2.2321\n",
            "Epoch [14/100], Loss: 2.2299\n",
            "Epoch [15/100], Loss: 2.2280\n",
            "Epoch [16/100], Loss: 2.2257\n",
            "Epoch [17/100], Loss: 2.2237\n",
            "Epoch [18/100], Loss: 2.2219\n",
            "Epoch [19/100], Loss: 2.2197\n",
            "Epoch [20/100], Loss: 2.2178\n",
            "Epoch [21/100], Loss: 2.2163\n",
            "Epoch [22/100], Loss: 2.2143\n",
            "Epoch [23/100], Loss: 2.2128\n",
            "Epoch [24/100], Loss: 2.2116\n",
            "Epoch [25/100], Loss: 2.2100\n",
            "Epoch [26/100], Loss: 2.2088\n",
            "Epoch [27/100], Loss: 2.2074\n",
            "Epoch [28/100], Loss: 2.2063\n",
            "Epoch [29/100], Loss: 2.2050\n",
            "Epoch [30/100], Loss: 2.2041\n",
            "Epoch [31/100], Loss: 2.2029\n",
            "Epoch [32/100], Loss: 2.2021\n",
            "Epoch [33/100], Loss: 2.2010\n",
            "Epoch [34/100], Loss: 2.1999\n",
            "Epoch [35/100], Loss: 2.1991\n",
            "Epoch [36/100], Loss: 2.1980\n",
            "Epoch [37/100], Loss: 2.1974\n",
            "Epoch [38/100], Loss: 2.1962\n",
            "Epoch [39/100], Loss: 2.1953\n",
            "Epoch [40/100], Loss: 2.1943\n",
            "Epoch [41/100], Loss: 2.1936\n",
            "Epoch [42/100], Loss: 2.1926\n",
            "Epoch [43/100], Loss: 2.1918\n",
            "Epoch [44/100], Loss: 2.1913\n",
            "Epoch [45/100], Loss: 2.1903\n",
            "Epoch [46/100], Loss: 2.1896\n",
            "Epoch [47/100], Loss: 2.1888\n",
            "Epoch [48/100], Loss: 2.1880\n",
            "Epoch [49/100], Loss: 2.1875\n",
            "Epoch [50/100], Loss: 2.1867\n",
            "Epoch [51/100], Loss: 2.1863\n",
            "Epoch [52/100], Loss: 2.1853\n",
            "Epoch [53/100], Loss: 2.1848\n",
            "Epoch [54/100], Loss: 2.1845\n",
            "Epoch [55/100], Loss: 2.1840\n",
            "Epoch [56/100], Loss: 2.1832\n",
            "Epoch [57/100], Loss: 2.1830\n",
            "Epoch [58/100], Loss: 2.1821\n",
            "Epoch [59/100], Loss: 2.1819\n",
            "Epoch [60/100], Loss: 2.1815\n",
            "Epoch [61/100], Loss: 2.1811\n",
            "Epoch [62/100], Loss: 2.1807\n",
            "Epoch [63/100], Loss: 2.1803\n",
            "Epoch [64/100], Loss: 2.1796\n",
            "Epoch [65/100], Loss: 2.1794\n",
            "Epoch [66/100], Loss: 2.1790\n",
            "Epoch [67/100], Loss: 2.1785\n",
            "Epoch [68/100], Loss: 2.1780\n",
            "Epoch [69/100], Loss: 2.1778\n",
            "Epoch [70/100], Loss: 2.1775\n",
            "Epoch [71/100], Loss: 2.1771\n",
            "Epoch [72/100], Loss: 2.1769\n",
            "Epoch [73/100], Loss: 2.1761\n",
            "Epoch [74/100], Loss: 2.1762\n",
            "Epoch [75/100], Loss: 2.1754\n",
            "Epoch [76/100], Loss: 2.1754\n",
            "Epoch [77/100], Loss: 2.1750\n",
            "Epoch [78/100], Loss: 2.1743\n",
            "Epoch [79/100], Loss: 2.1741\n",
            "Epoch [80/100], Loss: 2.1740\n",
            "Epoch [81/100], Loss: 2.1732\n",
            "Epoch [82/100], Loss: 2.1733\n",
            "Epoch [83/100], Loss: 2.1728\n",
            "Epoch [84/100], Loss: 2.1726\n",
            "Epoch [85/100], Loss: 2.1724\n",
            "Epoch [86/100], Loss: 2.1720\n",
            "Epoch [87/100], Loss: 2.1719\n",
            "Epoch [88/100], Loss: 2.1716\n",
            "Epoch [89/100], Loss: 2.1710\n",
            "Epoch [90/100], Loss: 2.1708\n",
            "Epoch [91/100], Loss: 2.1707\n",
            "Epoch [92/100], Loss: 2.1701\n",
            "Epoch [93/100], Loss: 2.1701\n",
            "Epoch [94/100], Loss: 2.1698\n",
            "Epoch [95/100], Loss: 2.1693\n",
            "Epoch [96/100], Loss: 2.1694\n",
            "Epoch [97/100], Loss: 2.1691\n",
            "Epoch [98/100], Loss: 2.1685\n",
            "Epoch [99/100], Loss: 2.1681\n",
            "Epoch [100/100], Loss: 2.1679\n",
            "learning. similarities: => \n",
            "learning. 1.0\n",
            "is 0.35057428\n",
            "i -0.04752455\n",
            "deep -0.15925804\n",
            "learn -0.53737956\n",
            "love -0.348886\n",
            "it 0.12817313\n",
            "to -0.03956433\n",
            "fascinating! -0.045585852\n",
            "\n",
            "\n",
            "is similarities: => \n",
            "learning. 0.35057428\n",
            "is 1.0\n",
            "i -0.15445493\n",
            "deep -0.45158994\n",
            "learn -0.49883336\n",
            "love -0.6244446\n",
            "it -0.14721546\n",
            "to 0.015991455\n",
            "fascinating! -0.17442752\n",
            "\n",
            "\n",
            "i similarities: => \n",
            "learning. -0.04752455\n",
            "is -0.15445493\n",
            "i 1.0\n",
            "deep 0.022608552\n",
            "learn -0.1477955\n",
            "love -0.2494188\n",
            "it -0.11560814\n",
            "to -0.5107286\n",
            "fascinating! 0.49425718\n",
            "\n",
            "\n",
            "deep similarities: => \n",
            "learning. -0.15925804\n",
            "is -0.45158994\n",
            "i 0.022608552\n",
            "deep 1.0\n",
            "learn -0.00064910855\n",
            "love 0.17332904\n",
            "it 0.10839442\n",
            "to -0.01775792\n",
            "fascinating! 0.46757725\n",
            "\n",
            "\n",
            "learn similarities: => \n",
            "learning. -0.53737956\n",
            "is -0.49883336\n",
            "i -0.1477955\n",
            "deep -0.00064910855\n",
            "learn 1.0000001\n",
            "love 0.3946535\n",
            "it -0.399001\n",
            "to -0.16960205\n",
            "fascinating! -0.5083494\n",
            "\n",
            "\n",
            "love similarities: => \n",
            "learning. -0.348886\n",
            "is -0.6244446\n",
            "i -0.2494188\n",
            "deep 0.17332904\n",
            "learn 0.3946535\n",
            "love 1.0\n",
            "it 0.35642794\n",
            "to -0.07376252\n",
            "fascinating! 0.009980341\n",
            "\n",
            "\n",
            "it similarities: => \n",
            "learning. 0.12817313\n",
            "is -0.14721546\n",
            "i -0.11560814\n",
            "deep 0.10839442\n",
            "learn -0.399001\n",
            "love 0.35642794\n",
            "it 1.0000001\n",
            "to 0.26657316\n",
            "fascinating! 0.591469\n",
            "\n",
            "\n",
            "to similarities: => \n",
            "learning. -0.03956433\n",
            "is 0.015991455\n",
            "i -0.5107286\n",
            "deep -0.01775792\n",
            "learn -0.16960205\n",
            "love -0.07376252\n",
            "it 0.26657316\n",
            "to 1.0000001\n",
            "fascinating! 0.03278235\n",
            "\n",
            "\n",
            "fascinating! similarities: => \n",
            "learning. -0.045585852\n",
            "is -0.17442752\n",
            "i 0.49425718\n",
            "deep 0.46757725\n",
            "learn -0.5083494\n",
            "love 0.009980341\n",
            "it 0.591469\n",
            "to 0.03278235\n",
            "fascinating! 0.99999994\n",
            "\n",
            "\n",
            "Word: learning., Embedding: tensor([-0.4792, -0.5856,  0.1212,  0.6214, -1.3003, -0.9602, -0.8019, -0.0760,\n",
            "         0.1314, -0.0685])\n",
            "Word: is, Embedding: tensor([ 0.1291, -1.1517, -0.9941, -0.4948, -1.1578,  0.4044, -0.4782,  0.7768,\n",
            "         0.9207,  0.1292])\n",
            "Word: i, Embedding: tensor([ 1.2786,  0.3190,  1.3745,  1.4075,  1.3728,  0.6653, -2.2449,  0.2670,\n",
            "         0.7777,  1.1650])\n",
            "Word: deep, Embedding: tensor([-0.3216,  2.0378, -1.3154,  0.0452,  2.1977, -1.6496, -1.2072, -0.1356,\n",
            "        -0.6978, -1.8576])\n",
            "Word: learn, Embedding: tensor([ 1.2138, -0.0495,  0.8617, -0.1038,  0.8274,  0.1814,  1.6973, -0.7719,\n",
            "        -0.2889, -0.9934])\n",
            "Word: love, Embedding: tensor([-1.3790,  0.5504,  0.3431,  0.3048,  1.1162,  0.0536,  1.4131, -1.2376,\n",
            "         0.2026, -0.1176])\n",
            "Word: it, Embedding: tensor([-2.2262, -0.4412,  0.0872,  0.3423,  0.8462, -0.6304,  0.1399,  0.9298,\n",
            "        -0.5266,  0.7260])\n",
            "Word: to, Embedding: tensor([-0.6008, -0.3096, -0.7020, -1.0621, -0.3952,  0.1010,  0.0827, -0.1448,\n",
            "        -1.9700,  0.4804])\n",
            "Word: fascinating!, Embedding: tensor([-1.0113,  0.2538, -0.2109,  0.6629,  1.3248,  0.2655, -1.4347,  0.7652,\n",
            "        -0.4289,  0.2415])\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1qHnmU8T6kw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}